---
title: "Notes of A/B Testing by Google"
output:
  html_document:
    toc: true
    toc_float:
      smooth_scroll: false
---

```{r, setup, echo=FALSE, results='hide', message=FALSE}
library(plyr)
library(tidyverse)
library(gridExtra)
knitr::opts_knit$set(root.dir = "~/Desktop/udacity/ab_testing_by_google/")
```

These are my notes for the [A/B Testing by Google](https://www.udacity.com/course/ab-testing--ud257) course I took on Udacity. It mainly consists of below 2 things.

1. My take on the quizes and projects.
2. My understanding on relevant statistics concepts.

This is being continuously updated.

# Lesson 1

## Important concepts

### What is "Pracitical Significance"?

What is "Pracitical Significance"?

### What is "Power"?

What is "Power"?

### What is the difference between one tailed test and two tailed test?

#### One tail test: do null hypothesis and alternative hypothesis need to be complementary?

I have always been confused by the use of one-tailed test. The part confuses me is that in many places I saw that $H_0$ and $H_a$ are not complementary, e.g., $H_0$: $\mu$ = 0, $H_a$: $\mu$ < 0, rather than $H_0$: $\mu$ >= 0, $H_a$: $\mu$ < 0,

After going through some searches, my current understaning is that $H_0$ and $H_a$ should be complementary for hypothesis testing to work. But we only need to calculate test statistic under the assumption when null hypothesis takes the equal sign (in previous example, that is when $\mu$ = 0). My understanding of one-tailed test vs two-tailed tests are as follows. Please note the example below is based on this [lecture note](https://math.mit.edu/~dav/05.dir/class17-prep-b.pdf). 

Usually the null distribution I have seen is t-distribution, z-distribution or binomial distribution. Here I use a binomial distribution as an example.

Suppose we want to test if a coin is biased towards head, then $H_a$ in this case should be "Probability of head, $\theta$, is greater than 0.5". Since this is a one sided test, then $H_0$ should be "Probability of head, $\theta$, is less than or equal to 0.5". In math expressions:

$$
H_0: \theta <= 0.5\\
H_a: \theta > 0.5
$$

The next step is to:

1. Suppose $H_0$ is true.
2. Pick and calculates the test statistic under the assumption of $H_0$ being true, e.g., x.

    + Test statistic is the number of heads in 100 throws.
  
3. Calculates the probability that test statistic is at least as extreme as x.

    + In this case "extreme" means only values that are too large, since null hypothesis assumes that coin is less likely to show head.

When $H_0$ is true, this means that $\theta$ can be 0.5, 0.49, 0.3, or any value that's less than/equal to 0.5. If that's the case, doesn't that mean we should calculate not one but many test statistics? My understanding is yes. However, we don't really need to calculate all the test statistics to determine what is extreme. We can illustrate it by plotting the density curves with a few different parameters.

```{r lesson1_example1}
df.l1.e1 = 
  data.frame(x1=1:100,
             d1=dbinom(1:100, 100, prob = 0.5),
             d2=dbinom(1:100, 100, prob = 0.45),
             d3=dbinom(1:100, 100, prob = 0.4),
             d4=dbinom(1:100, 100, prob = 0.35),
             d5=dbinom(1:100, 100, prob = 0.3)) %>%
  gather(key = "scenario", value = "density", -x1)

p1 = ggplot(df.l1.e1, aes(x1, density, color = scenario)) + 
  geom_line() + geom_point() +
  geom_vline(xintercept = qbinom(1-0.05,100,0.5))

p1
```

The 5 different curves represents 5 differnt density functions of a binomial distribution. They have the same sample size (100), but different probability of success. The vertical line shows the critical value (in this case 58) for d1 scenario. It means under d1 scenario, the probability of getting heads 58 or more times (i.e., at least as extreme as 58) is 5%.

As $\theta$ gets smaller, the density function moves more and more left. This means that the critial values for scenrarios, d2, d3, etc. will be smaller than 58. In other words, 58 is considered an extreme event for all these scenarios. Now we can see that if test statistic is 58 or more, then no matter what value $\theta$ takes, as long as it's less than/equal to 0.5, the test result will be an extreme event.

If the test statistic is, for example, 57, then we cannot reject $H_0$, because although under d2, d3... scenarios the result is extreme, it is not extreme when under d1 scenario. As a result, we cannot reject $H_0$.

So based on my current understaning, $H_0$ and $H_a$ should be complementary, but we only need to calculate test statistic under the assumption when null hypothesis takes the equal sign.

### Should we use t-test or z-test in hypothesis testing when comparing difference of means of 2 populations?

I have also been confused on this point. My understanding is **in theory we should use t-test**. However, if sample size is large enough, **using either t-test or z-test would be OK**.

Below is a example for clarification.

Suppose there are 2 populations, X and Y. The population means are $\mu_1$ and $\mu_2$, and the population variance are $Var_1$ and $Var_2$. Since in reality often we don't know population means, as a result we'd like to test if population means are truly the same.

First we define a few parameters and statistics.

``` {r lesson1_example2_pt0}
n.1 = 5; n.2 = 7 # define sample sizes
trials = 1e5 # define number of trials
mu.1 = 5; mu.2 = 5 # define population variances
sd.1 =2; sd.2 = 3 # define population standard deviations
```

For each population, we draw samples with size of `r n.1` and `r n.2`^[The reason we choose such small sample sizes is so that the difference between distributions of statistics are more obvious.], and we do this `r as.integer(trials)` times. This way we can get the sample mean and sample standard deviantion for all `r as.integer(trials)` draws.

```{r lesson1_example2_pt1}
s.ids = as.list(1:trials)

GenerateSample = function(id, s.size = 30, pop.mean = 1, pop.sd = 0){
  sampling = rnorm(s.size, pop.mean, pop.sd)
  return(data.frame(sample_id = id,
                    value = sampling))
}

df.samples.1 = ldply(s.ids, 
                     function(x) GenerateSample(x, s.size = n.1,
                                                pop.mean = mu.1, pop.sd = sd.1))
df.samples.2 = ldply(s.ids, 
                     function(x) GenerateSample(x, s.size = n.2,
                                                pop.mean = mu.2, pop.sd = sd.2))

df.s.stats.1 = 
  df.samples.1 %>% 
  group_by(sample_id) %>%
  summarise(sample_mean = mean(value),
            sample_sd = sd(value),
            sample_size = n())

df.s.stats.2 = 
  df.samples.2 %>% 
  group_by(sample_id) %>%
  summarise(sample_mean = mean(value),
            sample_sd = sd(value),
            sample_size = n())
```

The normal process of a hypothesis testing would be as follows:

1. Consturct $H_0$ ($\mu_1 = \mu_2$) and $H_a$ ($\mu_1 \neq \mu_2$).
2. Assuming $H_0$ is correct, we construct test statistics.
3. Calculate the statistics' values and p-value; compare with significance level.

The answer to the question of whether to use t-test or z-test actually depends on whether distributions of test statistics follow t-distribution or z-distribution. Ideally we would like to construct below statistic, where $Var_1$, $Var_2$ are population variances; $n_1$, $n_2$ are sample sizes:

$$
stat_1 = \dfrac {(\bar X - \bar Y) - (\mu_1 - \mu_2)}{\sqrt{\dfrac{Var_1}{n_1} + \dfrac{Var_2}{n_2}}}
$$

However, more often than not we don't know what are the values of population variances. In that case, there are 2 other statistics we can construct:

1. If we cannot assume $Var_1 = Var_2$, then we use below statistic, where sample variances $S_x^2$ and $S_y^2$ are used as estimates for $Var_1$ and $Var_2$: 
  
$$
stat_2 = \dfrac {(\bar X - \bar Y) - (\mu_1 - \mu_2)}{\sqrt{\dfrac{S_x^2}{n_1} + \dfrac{S_y^2}{n_2}}}
$$

2. If we can assume that $Var_1 = Var_2$, then we use below statistic.

$$
stat_3 = \dfrac{(\bar X - \bar Y) - (\mu_1 - \mu_2)}{\sqrt{\dfrac{S_{pool}^2}{n_1} + \dfrac{S_{pool}^2}{n_2}}} = \dfrac{(\bar X - \bar Y) - (\mu_1 - \mu_2)}{S_{pool}\sqrt{\dfrac{1}{n_1} + \dfrac{1}{n_2}}}
$$

where:

$$
S_{pool} = \sqrt{\dfrac{(n1-1)*S_x^2 + (n_2-1)*S_y^2}{(n1+n2-2)}}
$$

The $stat_2$ that we construct is a **[Welch's t-test](https://en.wikipedia.org/wiki/Welch's_t-test)**, and $stat_3$ a Student's t-test. What are the differnces? **Welch's t-test is used when population variances are unequal, while Student's t-test is used when population variances can be assumed to be equal**.

As we can see, the only difference between $stat_3$ and $stat_2$ is the way we estimate population variances. In the case of equal variance assumption, instead of estimating population variance separately, it's more appropriate to get a **[pooled variance](https://en.wikipedia.org/wiki/Pooled_variance)** and use that as an estimate of population variance for both samples.

The distribution of $stat_1$ is clearly z-distribution (i.e., standard normal distribution). What about $stat_2$ and $stat_3$? To find out, we plot out the distributions of the statistics to see what they look like. Here we assume we know that population variances are unequal and we plot for $stat_2$.

```{r lesson1_example2_pt2}
# Construct test statistic.
test.stat.1 = 
  (df.s.stats.1$sample_mean - df.s.stats.2$sample_mean) /
  (sqrt(sd.1^2 / n.1 + sd.2^2 / n.2))

# Construct test statistic.
test.stat.2 =
  (df.s.stats.1$sample_mean - df.s.stats.2$sample_mean) /
  (sqrt(df.s.stats.1$sample_sd^2 / n.1 + df.s.stats.2$sample_sd^2 / n.2))

# Putting empirical density along with standard normal and student's t
df.final = bind_rows(
  tibble(test = "stat_1", statistic_value = test.stat.1),
  tibble(test = "stat_2", statistic_value = test.stat.2),
  tibble(test = "standard_normal", statistic_value = rnorm(trials)),
  tibble(test = "t_dis_1", statistic_value = rt(trials, min(n.1, n.2) - 1)),
  tibble(test = "t_dis_2", statistic_value = rt(trials, (n.1 - 1) + (n.2 - 1)))
)

# Plots out both histograms
p.l1.1 = ggplot(df.final, aes(statistic_value)) +
  geom_histogram(data = df.final %>% filter(test == "stat_1"), 
                 binwidth = 1e-1, alpha = 0.2, fill = "red") +
  geom_histogram(data = df.final %>% filter(test == "stat_2"), 
                 binwidth = 1e-1, alpha = 0.2, fill = "blue") +
  labs(title = "Histograms of 2 statistics")

# Plots out both density curves
p.l1.2 = ggplot(df.final %>% 
                  filter(test %in% c("stat_1", "stat_2"))) +
  geom_density(aes(statistic_value, color = test, fill = test), alpha = 0.2) +
  labs(title = "Density curves of 2 statistics")

grid.arrange(p.l1.1, p.l1.2, nrow = 2)
```

We can see that the 2 distributions differ slightly. Next we compare them with a standard normal distribution and a t-distribution respectively.

```{r lesson1_example2_pt3}
# Compare statistic with standard normal distribution
p.l1.3 = ggplot(df.final %>% 
                  filter(test %in% c("stat_1", "standard_normal"))) +
  geom_density(aes(statistic_value, color = test, fill = test), alpha = 0.2)

p.l1.3
```

``` {r lesson1_example2_pt4}
# Compare statistic with student t distribution
p.l1.4 = ggplot(df.final %>% 
                  filter(test %in% c("stat_2", "t_dis_1", "t_dis_2"))) +
  geom_density(aes(statistic_value, color = test, fill = test), alpha = 0.2)

p.l1.4
```

As we can see. The $stat_1$ statistic follows a standardized normal distribution, and $stat_2$ statistic follows a t distribution which, based on the chart, has degrees of freedom approximately equal to `r (n.1 - 1) + (n.2 - 1)`. In hypothesis testing, we can use `r min(n.1 - 1, n.2 - 1)` degress of freedom as a conservative choice. The actual formula of degrees of freedom is mentioned later.

If sample size were larger (e.g., 100 for each group), then both $stat_1$ and $stat_2$ would follow standard normal distribution.

As a conclusion, in cases where we don't know population variances, then in theory we should use t-test. However, when sample sizes are large enough, **using either t-test or z-test would be OK**.

#### How to calculate the degrees of freedom of a t test statistic?

The way to calculate degrees of freedom differs depends on the t test statistic we constructed.

* For Student's t-test, degrees of freedom is $n_1 + n_2 - 2$.^[https://en.wikipedia.org/wiki/Student%27s_t-test#Independent_two-sample_t-test]

* For Welch's t-test, degrees of freedom can be approximated using below formula^[https://en.wikipedia.org/wiki/Student%27s_t-test#Independent_two-sample_t-test]:

$$
d.f. \approx \dfrac {(\dfrac{S_x^2}{N_1} + \dfrac{S_y^2}{N_2})^2}{\dfrac{S_x^4}{N_1^2(N_1-1)} + \dfrac{S_y^4}{N_2^2(N_2-1)}}
$$

Since $stat_2$ is a Welch's t-test, we can calculate the d.f. for each of the sample that's drawn.

``` {r lesson1_example2_pt5}
degree.of.freedom.stat.2 = 
  (
    (df.s.stats.1$sample_sd)^2 / df.s.stats.1$sample_size + 
    (df.s.stats.2$sample_sd)^2 / df.s.stats.2$sample_size
  )^2 /
  (
    (df.s.stats.1$sample_sd)^4 /
      ((df.s.stats.1$sample_size)^2*(df.s.stats.1$sample_size - 1)) +
    (df.s.stats.2$sample_sd)^4 /
      ((df.s.stats.2$sample_size)^2*(df.s.stats.2$sample_size - 1))
  )

p.l1.5 = 
  ggplot(tibble(df = degree.of.freedom.stat.2), aes(df)) +
  geom_histogram(binwidth = 1)

p.l1.5
```

We can see that d.f. ranges between `r round(min(degree.of.freedom.stat.2),2)` and `r max(degree.of.freedom.stat.2)`. The former is approximately the smaller of $n_1 - 1$ and $n_2 - 1$, and the latter is $n_1 + n_2 - 2$.

### What is click-through-probability?

What is click-through-probability?

* For each page view, match it to all the child clicks, and count at most 1 click
* Sum of number of clicks divided by sum of page views

## 22

* $\alpha$ is type I error
* $\beta$ is type II error
* $Power = 1 - \beta$

Power is also called **sensitivity**, in the context of binary classification.

### Important concepts

#### How is sample size calculated?

To determine a minimum sample size, we need to think about what are the different things that can determine a sample size for an A/B testing? Often there are a few things like below:

* Type I error is 5%.
* Power is 80%. Since Power = 1 - Type II error, so in other words, Type II error = 20%.
* A Minimum Detectable Effect (MDE) of some value $X$; in the course this was called **"practical significance level" and "substantive significance level"**. What is this specifically? Suppose we are testing a metric. We have $\mu_{con}$ and $\mu_{exp}$ which are the population mean of control group and experiment group respectively. We define delta as $\delta = \hat \mu_{exp} - \hat \mu_{con}$, which is the difference of the sample means. So $\delta$ follows a normal distribution. If $H_0$ is true, then $\delta$ follows a normal distribution with mean equal to zero. If $H_0$ is not true, then $\delta$ follows a normal distribution with mean equal to MDE.

To better illustrate the idea, we use a one-sided test and draw below charts.

```{r lesson1_sec22_code1}
delta.1 = seq(-5,8,by = 0.1)

df.l1.s22.1 = tibble(delta = c(delta.1, delta.1),
                     density = c(dnorm(delta.1),
                                 dnorm(delta.1, mean = 3)),
                     scenario = c(rep("H0 is true", length(delta.1)), 
                                  rep("H0 is false", length(delta.1))))

p.l1.6 = 
  ggplot(df.l1.s22.1, aes(delta, density, fill = scenario, color = scenario)) + 
  geom_line() +
  geom_vline(xintercept=0, color = "blue") + 
  geom_vline(xintercept = 3, color = "red") +
  geom_vline(xintercept = 1.64, color = "green")

p.l1.6
```

The left curve is the density curve when $H_0$ is true, and right curve is the density curve when it's not. The blue and red vertical line represents the mean of the 2 density curves. The blue line represents 0, and red line represents MDE. The green line represents our decision cutoff point, i.e., if a measured value is greater than green line, then we reject $H_0$. The area (1) under red curve, and (2) right of green line represents Power. The area (1) under red curve, and (2) left of green line represents Type II error.

As we can see, if we decided on alpha, beta and MDE beforehand, then that means blue line and red line are now fixed. To keep $\alpha = 0.05$ as sample size gets larger, the green line will move to the left and closer to blue line. This is because the standard deviation of both blue and red curves will become smaller. This also means Power becomes larger. In other words, we can increase sample size to a amount so that Power, or $1 - \beta$, increases to 80%. And that's why we can determine a sample size if we have the $\alpha$, $\beta$, and MDE.

There's actually a formula for the size calculation. Details on that can be found on this very helpful article from twitter's engineering blog^[See the link in Appendix].

Other relevant resources to this subsection can be found at [Khan Academy's significance testing lessons](https://www.khanacademy.org/math/ap-statistics/tests-significance-ap/error-probabilities-power/v/introduction-to-power-in-significance-tests).

## 24

How does practical significance affect page size?

* A helpful article on this topic is from twitter's enginerring blog^[See the link in Appendix].

## 25
```{r, lesson1_sec25}
n.cont = 10072
x.cont = 974
p.cont = x.cont / n.cont

n.exp = 9886
x.exp = 1242
p.exp = x.exp / n.exp

p.all = (x.cont + x.exp) / (n.cont + n.exp)

d.hat = p.exp - p.cont
moe = qnorm(0.975) * sqrt(p.all * (1-p.all) * (1 / n.cont + 1 / n.exp))

bdry = d.hat + c(-1, 1) * moe
round(bdry, 4)
```

# Lesson 3

## 16

For below 3 distributions: 

* Poisson (understand this first)
* Exponential
* Pareto

We wanted to understand:

* What phenomena do they describe?
* What are their density formulae?
* How to plot them out in R?

## 22

Here is the quiz data for you to copy and paste: [87029, 113407, 84843, 104994, 99327, 92052, 60684]

```{r, lesson3_sec22}
s.1 = c(87029, 113407, 84843, 104994, 99327, 92052, 60684)
se = sd(s.1) / sqrt(length(s.1))

result.22 =  mean(s.1) + c(-1, 1) * qnorm(0.975) * se
result.22
```

## 27

Data contains click-through-probability values from 40 A/A Tests or bootstrap samples.

Notes:

* To calculate empirical variance, for each group, sample size of each experiment needs to remain the same. This is because standard error is dependent on sample size.

```{r, lesson3_sec27}
df.q27 = read.table("data/data_l3_q27.csv", header = TRUE, 
                    na.strings = c(""), stringsAsFactors = FALSE)

df.q27 = df.q27 %>%
  mutate(diff = Group.1 - Group.2)
# 1. calculate the standard deviation of the differences, 
# and assume metric is normally distributed
bdry.22.1 = round(mean(df.q27$diff) + qnorm(0.975) * sd(df.q27$diff) * c(-1, 1),
                  4)

# 2. calculate an empirical confidence interval,
# making no assumptions about the distribution
diffs = sort(df.q27$diff, decreasing = FALSE)
bdry.22.2 = c(diffs[2], diffs[39])

bdry.22.1
bdry.22.2
```

# Lesson 4

Important concepts:

* What is unit of diversion? It is by what unit is the sampling being done. For example, suppose there is an A/B testing on the impact of a website changing its UI after user login. For this testing:

    + If the unit of diversion is "Cookie", then events will be sampled at cookie level. i.e., events under the same cookie will always be in the same group. This ensures the same cookie gets the consistent experience (whether it be Control or Experiment).
    + If the unit of diversion is "User ID", then events will be sampled at user id level. The rationale here is similar to "Cookie".
    + If the unit of diversion is "Event", then events will be sampled at event level. This means that the same cookie or user id may get into control group one time, and experiment group the next time.

In this case, a unit of diversion can be either "Cookie" or "User ID", but cannot be "Event".

## 5

What are the relationships among "event", "cookie", and "user id"?

My understanding is that:

* A user id can have multiple events.
* A cookie can have multiple events.
* A cookie is unique to each browser, ananymous, and do not require user log in.

## 9

Unit of diversion should be "larger" than unit of analysis.

Why empirical variability is different from analytical variability when unit of analysis is different from unit of diversion?

*Answer:*

This is because why unit of diversion is "larger" than unit of analysis, the sampled events are more correlated with each other than when unit of diversion equal to unit of analysis.

Use click-through rate as an example:

1. Unit of analysis is pageview.
2. Choosing cookie as unit of diversion means that events (a pageview) are sampled not at event level, but at cookie level.
3. As a result, sampled pageviews are not independent from each other, because one cookie can have multiple pageviews. All pageviews in the cookie are correlated with each other, and not independent.

Why variance is larger when sampling is not independent?

## 10

3 types of experiments:

* Inter-user experiment (main topic in a/b testing)
* Intra-user experiment
* Interleaved experiment (usually relevant in re-ordering a list)

## 12

```{r, lesson4_sec12}
# new zealand
nz.n.cont = 6021
nz.x.cont = 302
nz.p.cont = nz.x.cont / nz.n.cont

nz.n.exp = 5979
nz.x.exp = 374
nz.p.exp = nz.x.exp / nz.n.exp

nz.p.all = (nz.x.cont + nz.x.exp) / (nz.n.cont + nz.n.exp)
nz.se = sqrt(nz.p.all * (1 - nz.p.all) * (1 / nz.n.cont + 1 / nz.n.exp))

# other
other.n.cont = 5e4
other.x.cont = 2500
other.p.cont = other.x.cont / other.n.cont

other.n.exp = 5e4
other.x.exp = 2500
other.p.exp = other.x.exp / other.n.exp

# global
gl.n.cont = nz.n.cont + other.n.cont
gl.x.cont = nz.x.cont + other.x.cont
gl.p.cont = gl.x.cont / gl.n.cont

gl.n.exp = nz.n.exp + other.n.exp
gl.x.exp = nz.x.exp + other.x.exp
gl.p.exp = gl.x.exp / gl.n.exp

gl.p.all = (gl.x.cont + gl.x.exp) / (gl.n.cont + gl.n.exp)

# question 1
gl.se = sqrt(gl.p.all * (1 - gl.p.all) * (1 / gl.n.cont + 1 / gl.n.exp))
gl.se

# question 2
(1 - pnorm(abs(nz.p.cont - nz.p.exp) / nz.se)) * 2 < 0.05
(1 - pnorm(abs(gl.p.cont - gl.p.exp) / gl.se)) * 2 < 0.05

```

## 18

Important concepts:

* How is number of pages calculated?
    * How is power formula derived?
    
A helpful article on this topic is from twitter's engineering blog^[See the link in Appendix].

# Lesson 5

## 6

If the population's portion of control group is 0.5, then the sample portion follows a normal distribution $N(0.5, 0.5 * (1-0.5) / n)$, where $n$ is the sample size.

```{r, lesson5_sec6}
n.cont.q5.6 = 15348
n.exp.q5.6 = 15312

p.hat = n.cont.q5.6 / (n.exp.q5.6 + n.cont.q5.6)
round(
  0.5 + 
  c(-1, 1) * qnorm(0.975) * 
  sqrt(0.5 * 0.5 * 1 / (n.exp.q5.6 + n.cont.q5.6)), 4)

```

## 9

What's the reason that empirically standard error can be derived?
$$\frac{0.0035}{\sqrt{\frac{1}{10000} + \frac{1}{10000}}} = 
\frac{se}{\sqrt{\frac{1}{7370} + \frac{1}{7270}}}$$

```{r, lesson5_sec9_1}
Xs_cont = c(196, 200, 200, 216, 212, 185, 225, 187, 205, 211, 192, 196, 223, 192)
Ns_cont = c(2029, 1991, 1951, 1985, 1973, 2021, 2041, 1980, 1951, 1988, 1977, 2019, 2035, 2007)
Xs_exp = c(179, 208, 205, 175, 191, 291, 278, 216, 225, 207, 205, 200, 297, 299)
Ns_exp = c(1971, 2009, 2049, 2015, 2027, 1979, 1959, 2020, 2049, 2012, 2023, 1981, 1965, 1993)

x.total.cont = sum(Xs_cont)
n.total.cont = sum(Ns_cont)
x.total.exp = sum(Xs_exp)
n.total.exp = sum(Ns_exp)

ctr.cont = x.total.cont / n.total.cont
ctr.exp = x.total.exp / n.total.exp

# calculate empirical variance
emp.se = 0.0062 * sqrt(1/n.total.cont + 1/n.total.exp) / sqrt(1/5000 + 1/5000)

# calculate confidence intervals
emp.ci = round((ctr.exp - ctr.cont) + c(-1, 1) * qnorm(0.975) * emp.se, 4)
```

Zero is not included in the confidence interval, so the result is significant. However, $d_{min}(0.01)$ is included in it, so we can't be 95% confident the difference are substantive.

```{r, lesson5_sec9_2}
# sign test
## define a function
calc.sign.test = function(n.success, size, prob.success, is.two.tail){
  # calculates p value for a binomial distribution
  # arg:
  #     n.success: an integer, number of successes
  #     size: an integer, number of trials
  #     prob.success: a double, probability of "success" on each trial
  #     is.two.tail: a logical, TRUE or FALSE
  # return:
  #     p-value: probability of events at least as extreme as n.success
  if(is.two.tail){
    return(round((1 - pbinom(n.success - 1, size = size, prob = prob.success)) + 
             pbinom(size - n.success, size = size, prob = prob.success), 4))
  }
}

## define "success" as experiment group has higher CTR than control group
n.success = sum((Xs_exp / Ns_exp - Xs_cont / Ns_cont) > 0)
size = length(Xs_cont)

## calculate the 2 tail p value
calc.sign.test(n.success, size, prob.success = 0.5, is.two.tail = TRUE)

```

After going to the [online calculator](https://www.graphpad.com/quickcalcs/binomial2/), below are the results. The 2 tail p value is the same as what is calculated above.

Sign and binomial test:

* Number of "successes": 9 
* Number of trials (or subjects) per experiment: 14 
* Sign test. If the probability of "success" in each trial or subject is 0.500, then:
* The one-tail P value is 0.2120 
* This is the chance of observing 9 or more successes in 14 trials.
* The two-tail P value is 0.4240 
* This is the chance of observing either 9 or more successes, or 5 or fewer successes, in 14 trials.

## 11

CTR is higher in experiment group for both new and experienced users, but overall CTR is lower in the experiment group.

Simpson's Paradox explained using vectors: [explanation](https://en.wikipedia.org/wiki/Simpson%27s_paradox#Vector_interpretation)

## 12

Refernce on an important concept: [Multiple comparisons problem](https://en.wikipedia.org/wiki/Multiple_comparisons_problem)

## 13

Why is overall alpha overestimated if the metrics are not independent?

```{r, lesson5_sec13}
# define a function to calculate probaility of at least one false positive
calc.fp = function(size, ci){
  # arg
  # size: an integer, number of metrics
  # ci: a double, confidence interval
  # return
  # result: a double, probability
  return(round(1 - ci ^ size, 3))
}

calc.fp(10, 0.95)
calc.fp(10, 0.99)

```

## 14

An important concept related to this quiz is **Bonferroni correction**. In a nutshell, Bonferroni Correction gives each hypothesis test a signficance level of $\alpha / m$, where $\alpha$ is the overall signifance level, and $m$ is the number of tests. The **[Wikipedia page of Bonferroni Correction]((https://en.wikipedia.org/wiki/Bonferroni_correction))** shows that this ensure **[Family Wise Error Rate](https://en.wikipedia.org/wiki/Family-wise_error_rate)** is less than $\alpha$.

I feel that below notation makes the proof more easily understood for me.

$$
P\{\bigcup_{i=1}^{m_0} (|t_i| \geq |t_i^*|) \} \leq \bigcup_{i=1}^{m_0} P(|t_i| \geq |t_i^*|) = m_0 * \frac{\alpha}{m} \leq m * \frac{\alpha}{m} = \alpha
$$

The difference is that now we states that we are looking for the probability of test statistics at least as extreme as critical values, which is $\alpha / m$. In the Wikipedia page it states the probability as $P(p_i <= \alpha / m)$.

```{r, lesson5_sec14}
df.q5.14 = data.frame(metric = c("ctp to cource overview",
                                 "avg time spent reading overview",
                                 "prob of enrolling",
                                 "avg time in classroom during first week"),
                      delta = c(0.03, -0.5, 0.01, 10),
                      se = c(0.013, 0.21, 0.0045, 6.85))

df.q5.14 =
  df.q5.14 %>%
  mutate(p.value = (1 - pnorm(abs(delta) / se)) * 2,
         sig.flag = ifelse(p.value < 0.05, TRUE, FALSE),
         sig.flag.bonf = ifelse(p.value < 0.05 / 4, TRUE, FALSE))
```

## 15

A few important concepts and their wikipedia references:

- [Family wise error rate](https://en.wikipedia.org/wiki/Family-wise_error_rate)
- [False discovery rate](https://en.wikipedia.org/wiki/False_discovery_rate)
- [Holm-Bonferroni method](https://en.wikipedia.org/wiki/Holmâ€“Bonferroni_method#Example)

## 16

An OEC is an overall evaluation criterion.

How do we decide if we want to launch the change or not?

*Answer:*

We need to ask ourselves a few questions:

* Do we have statistically significant and practically significant results?
* What the change has actually done with regard to user experience?
* Is it worth it?

It's important to remember that the end goal is to **make that recommendation based on our judgements**. Designing experiments, running experiments, etc., are all signposts towards the end goal.

# Final Project

## Quiz 4

Below are rough estimates of the baseline values for these metrics:

* Unique cookies to view course overview page per day:	40000
* Unique cookies to click "Start free trial" per day:	3200
* Enrollments per day:	660
* Click-through-probability on "Start free trial":	0.08
* Probability of enrolling, given click:	0.20625
* Probability of payment, given enroll:	0.53
* Probability of payment, given click: 0.1093125

```{r, final_project_quiz4}
se.gc = sqrt(0.20625 * (1-0.20625) / (5000 * 0.08))
se.r = sqrt(0.53 * (1-0.53)/(5000 * 0.08 * 0.20625))
se.nc = sqrt(0.1093125 * (1-0.1093125) / (5000 * 0.08))

lapply(list(gross_converion = se.gc, retention = se.r,
            net_conversion = se.nc), function(x) round(x, 4))
```

## Quiz 5

Used this [calculator](http://www.evanmiller.org/ab-testing/sample-size.html)

```{r, final_project_quiz5}
n.gc = 25835; n.r = 39115; n.rc = 27411;
# n.total.pg = (max(c(n.gc, n.r / 0.20625, n.rc)) / 0.08) * 2
n.total.pg = (max(c(n.gc, n.rc)) / 0.08) * 2

```

## Quiz 6

```{r, final_project_quiz6}
p = 1

n.total.pg / (40000 * p)

```

## Quiz 7

```{r, final_project_quiz7}
df.fp.cont = read.csv("data/Final Project Results - Control.csv",
                      header = TRUE, na.strings = c(""), stringsAsFactors = FALSE)

df.fp.exp = read.csv("data/Final Project Results - Experiment.csv",
                      header = TRUE, na.strings = c(""), stringsAsFactors = FALSE)

df.fp.cont = df.fp.cont %>%
  mutate(Date = as.Date(Date, format = "%a, %b %d"))

df.fp.exp = df.fp.exp %>%
  mutate(Date = as.Date(Date, format = "%a, %b %d"))
```

### Number of cookies

```{r, final_project_quiz7_1}
n.fp.nc.cont = sum(df.fp.cont$Pageviews)
n.fp.nc.exp = sum(df.fp.exp$Pageviews)

p.fp.nc = round(
  n.fp.nc.cont / (n.fp.nc.cont + n.fp.nc.exp), 4)

ci.fp.nc = round(
  0.5 + 
  c(-1, 1) * qnorm(0.975) * sqrt(0.5 * (1-0.5) / (n.fp.nc.cont + n.fp.nc.exp)),
  4)

cat("\nobserved: ", p.fp.nc, 
    "\nupper: ", max(ci.fp.nc), "\tlower: ", min(ci.fp.nc))

if(p.fp.nc <= min(ci.fp.nc) | p.fp.nc >= max(ci.fp.nc)) {
  cat("significant")
} else {
  cat("not significant")
}

```

### Number of clicks on "start free trial"

```{r, final_project_quiz7_2}
n.fp.clk.cont = sum(df.fp.cont$Clicks)
n.fp.clk.exp = sum(df.fp.exp$Clicks)

p.fp.clk = round(
  n.fp.clk.cont / (n.fp.clk.cont + n.fp.clk.exp), 4)

ci.fp.clk = round(
  0.5 + 
  c(-1, 1) * qnorm(0.975) * sqrt(0.5 * (1-0.5) / (n.fp.clk.cont + n.fp.clk.exp)),
  4)

cat("\nobserved: ", p.fp.clk, 
    "\nupper: ", max(ci.fp.clk), "\tlower: ", min(ci.fp.clk))

if(p.fp.clk <= min(ci.fp.clk) | p.fp.clk >= max(ci.fp.clk)) {
  cat("significant")
} else {
  cat("not significant")
}

```

### Click-through-probability on "start free trial"

Why the answer doesn't consider this metric as an invariant metric?

```{r, final_project_quiz7_3}
p.fp.ctp.cont = sum(df.fp.cont$Clicks) / sum(df.fp.cont$Pageviews)
p.fp.ctp.exp = sum(df.fp.exp$Clicks) / sum(df.fp.exp$Pageviews)
p.fp.ctp.both = 
  sum(df.fp.cont$Clicks, df.fp.exp$Clicks) / 
  sum(df.fp.cont$Pageviews, df.fp.exp$Pageviews)

ci.fp.ctp = 
  round((p.fp.ctp.exp - p.fp.ctp.cont) +
          c(-1, 1) * qnorm(0.975) * 
          sqrt(p.fp.ctp.both *
                 (1 - p.fp.ctp.both) *
                 (1 / n.fp.nc.cont + 1 / n.fp.nc.exp)), 5)

p.fp.ctp.delta = round(p.fp.ctp.exp - p.fp.ctp.cont, 5)

cat("\nobserved: ", p.fp.ctp.delta, 
    "\nupper: ", max(ci.fp.ctp), "\tlower: ", min(ci.fp.ctp))

if(p.fp.ctp.delta <= min(ci.fp.ctp) | p.fp.ctp.delta >= max(ci.fp.ctp)) {
  cat("significant")
} else {
  cat("not significant")
}
```

## Quiz 8

Remove rows that don't have enrollments or payments data.

```{r, final_project_quiz8}
df.fp.cont.2 = df.fp.cont %>% na.omit()
df.fp.exp.2 = df.fp.exp %>% na.omit()
```

### Gross Conversion

Why does the answer consider this as practically significant?

```{r, final_project_quiz8_1}
n.fp.enroll.cont = sum(df.fp.cont.2$Enrollments)
n.fp.enroll.exp = sum(df.fp.exp.2$Enrollments)

n.fp.clk.cont.2 = sum(df.fp.cont.2$Clicks)
n.fp.clk.exp.2 = sum(df.fp.exp.2$Clicks)

p.fp.gc.cont = n.fp.enroll.cont / n.fp.clk.cont.2
p.fp.gc.exp = n.fp.enroll.exp / n.fp.clk.exp.2
p.fp.gc.both = 
  (n.fp.enroll.cont + n.fp.enroll.exp) / 
  (n.fp.clk.cont.2 + n.fp.clk.exp.2)

ci.fp.gross.conv = 
  round((p.fp.gc.exp - p.fp.gc.cont) + c(-1, 1) * qnorm(0.975) *
          sqrt(p.fp.gc.both * 
                 (1-p.fp.gc.both)*
                 (1 / n.fp.clk.cont.2 + 1 / n.fp.clk.exp.2)), 4)

ci.fp.gross.conv
```

### Net Conversion

```{r, final_project_quiz8_2}
n.fp.payment.cont = sum(df.fp.cont.2$Payments)
n.fp.payment.exp = sum(df.fp.exp.2$Payments)

p.fp.net.conv.cont = n.fp.payment.cont / n.fp.clk.cont.2
p.fp.net.conv.exp = n.fp.payment.exp / n.fp.clk.exp.2
p.fp.net.conv.both = 
  (n.fp.payment.cont + n.fp.payment.exp) / 
  (n.fp.clk.cont.2 + n.fp.clk.exp.2)

ci.fp.net.conv = 
  round((p.fp.net.conv.exp - p.fp.net.conv.cont) + c(-1, 1) * qnorm(0.975) *
          sqrt(p.fp.net.conv.both * 
                 (1-p.fp.net.conv.both) *
                 (1 / n.fp.clk.cont.2 + 1 / n.fp.clk.exp.2)), 4)

ci.fp.net.conv
```

## Quiz 9

```{r, final_project_quiz9}
## define a function
calc.sign.test = function(n.success, size, prob.success, is.two.tail){
  # calculates p value for a binomial distribution
  # arg:
  #     n.success: an integer, number of successes
  #     size: an integer, number of trials
  #     prob.success: a double, probability of "success" on each trial
  #     is.two.tail: a logical, TRUE or FALSE
  # return:
  #     p-value: probability of events at least as extreme as n.success
  if(is.two.tail){
    return(round((1 - pbinom(n.success - 1, size = size, prob = prob.success)) + 
             pbinom(size - n.success, size = size, prob = prob.success), 4))
  }
}
```

### Gross Conversion

```{r, final_project_quiz9_1}
## define "success" as experiment group has higher values than control group
n.fp.gc.success = sum((df.fp.cont.2$Enrollments / df.fp.cont.2$Clicks - 
                   df.fp.exp.2$Enrollments / df.fp.exp.2$Clicks) > 0)

## calculate the 2 tail p value
calc.sign.test(n.success = n.fp.gc.success, size = dim(df.fp.cont.2)[1],
               prob.success = 0.5, is.two.tail = TRUE)

```

### Net Conversion

```{r, final_project_quiz9_2}
## define "success" as experiment group has higher values than control group
n.fp.nc.success = sum((df.fp.cont.2$Payments / df.fp.cont.2$Clicks -
                         df.fp.exp.2$Payments / df.fp.exp.2$Clicks) > 0)

## calculate the 2 tail p value
calc.sign.test(n.success = n.fp.nc.success, size = dim(df.fp.cont.2)[1],
               prob.success = 0.5, is.two.tail = TRUE)

```

## Recommendation

My recommendation is to not launch this experiment. The reasons are:

1. There are no evidence suggesting Net Conversion rate is significantly different. As a result, revenue would not likely to increase after launching the change.
2. Launching the change is likely to add development and maintenance cost.
3. Based on above 2 reasons, cost would exceed revenue after launching the change.

# Appendix
1. [A note on this course on "towards data science"](https://towardsdatascience.com/a-summary-of-udacity-a-b-testing-course-9ecc32dedbb1)

2. [Twitter Engineering's article on determining page size](https://blog.twitter.com/engineering/en_us/a/2016/power-minimal-detectable-effect-and-bucket-size-estimation-in-ab-tests.html)
